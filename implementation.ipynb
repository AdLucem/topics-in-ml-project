{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Policy Gradient Changes When You Add Action-Dependent Factorized Baselines\n",
    "\n",
    "So first we have our list of imports, and a few utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "def pad(arr, size):\n",
    "    original_size = arr.shape[1]\n",
    "    padding_size = size - original_size\n",
    "    zeroes = np.zeros([1, padding_size])\n",
    "    return np.concatenate([arr, zeroes], axis=1)\n",
    "\n",
    "def unpad(arr, size):\n",
    "    return arr[0:, 0:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Class\n",
    "\n",
    "First we define all the reinforcement-learning hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, env, sess):\n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = .995\n",
    "        self.gamma = .95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the actor model in Keras. \n",
    "\n",
    "Remember, we calculate error on the actor network using a \"target\" acquired from the critic network weights. That is, we use the Chain rule: find the gradient of chaging the actor network params in getting closest to the final value network predictions, i.e. de/dA Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.memory = deque(maxlen=2000)\n",
    "        self.actor_state_input, self.actor_model = self.create_actor_model()\n",
    "        _, self.target_actor_model = self.create_actor_model()\n",
    "\n",
    "        self.actor_critic_grad = tf.placeholder(\n",
    "            tf.float32,\n",
    "            [None, self.env.observation_space.shape[0]])\n",
    "        \n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(\n",
    "            self.actor_model.output,\n",
    "            actor_model_weights,\n",
    "            -self.actor_critic_grad)\n",
    "        # dC/dA (from actor)\n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the critic model, which is relatively easier to train, as the target score (expected return for action) is calculated with respect to the simulated episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.critic_state_input, self.critic_action_input, \\\n",
    "            self.critic_model = self.create_critic_model()\n",
    "        _, _, self.target_critic_model = self.create_critic_model()\n",
    "\n",
    "        self.critic_grads = tf.gradients(self.critic_model.output,\n",
    "                                         self.critic_action_input)\n",
    "        # where we calcaulte de/dC for feeding above\n",
    "\n",
    "        # Initialize for later gradient calculations\n",
    "        self.sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our actor model is a 3-layer fully connected dense neural network in Keras, taken from this blogpost: https://towardsdatascience.com/reinforcement-learning-w-keras-openai-actor-critic-models-f084612cfd69."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_actor_model(self):\n",
    "\n",
    "        state_input = Input(shape=self.env.observation_space.shape)\n",
    "        h1 = Dense(24, activation='tanh')(state_input)\n",
    "        h2 = Dense(24, activation='tanh')(h1)\n",
    "        h3 = Dense(24, activation='tanh')(h2)\n",
    "        output = Dense(self.env.observation_space.shape[0], activation='tanh')(h3)\n",
    "\n",
    "        model = Model(input=state_input, output=output)\n",
    "        adam = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our critic model has two 24-unit input layers: one for the state, and one for the action values. \n",
    "\n",
    "For the environment we're using- OpenAI gym's Bipedal Walker environment- the action is actually a vector with only four dimensions. To make the dimensions of the critic network's layers match the dimensions of the actor network's layers (this will come in useful later), we pad the action vector with zeroes to make it a 24-dimension vector.\n",
    "\n",
    "The output of the critic model is only one number- the score, or predicted state-action value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def create_critic_model(self):\n",
    "        state_input = Input(shape=self.env.observation_space.shape,\n",
    "                            name=\"state_input\")\n",
    "        state_h1 = Dense(24, activation='relu',\n",
    "                         name=\"state_h1\")(state_input)\n",
    "\n",
    "        action_input = Input(shape=self.env.observation_space.shape,\n",
    "                             name=\"action_input\")\n",
    "        action_h1 = Dense(24, name=\"action_h1\")(action_input)\n",
    "\n",
    "        merged = Add(name=\"merged\")([state_h1, action_h1])\n",
    "        merged_h1 = Dense(24, activation='relu',\n",
    "                          name=\"merged_h1\")(merged)\n",
    "        output = Dense(1, activation='relu',\n",
    "                       name=\"output\")(merged_h1)\n",
    "\n",
    "        model = Model(input=[state_input, action_input], output=output)\n",
    "\n",
    "        adam = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, action_input, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training actor network using:\n",
    "\n",
    "* Our sample set\n",
    "* The action predicted by current actor network, given the action from our sample set\n",
    "* The critic network's scoring of our current actor model's predicted action\n",
    "\n",
    "The gradient backpropped onto the actor network is: dE/dA = dE/dC * dC/dA\n",
    "* E: error between critic's scoring of episode-sample action and predicted action\n",
    "* C: critic network weights\n",
    "* A: actor network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _train_actor(self, samples):\n",
    "\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, _ = sample\n",
    "            predicted_action = self.actor_model.predict(cur_state)\n",
    "\n",
    "            grads = self.sess.run(self.critic_grads, feed_dict={\n",
    "                self.critic_state_input: cur_state,\n",
    "                self.critic_action_input: predicted_action\n",
    "            })[0]\n",
    "\n",
    "            self.sess.run(self.optimize, feed_dict={\n",
    "                self.actor_state_input: cur_state,\n",
    "                self.actor_critic_grad: grads\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training critic network using our current (Si, Ai) -> Ri pairs.\n",
    "\n",
    "This code displays the action-dependent-baseline altered critic network. The section between hashed lines shows the calculation of predicted reward as a mean of critic obtained from averaging out a dimension of the action, for dimensions in {1..M}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _train_critic(self, samples):\n",
    "\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, done = sample\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model.predict(new_state)\n",
    "                target_action_unpad = unpad(target_action, 4) \n",
    "                avg_action = np.mean(target_action_unpad)\n",
    "\n",
    "                ###########################################\n",
    "                future_reward = 0\n",
    "                for i in range(len(target_action_unpad)):\n",
    "                    temp = target_action[i]\n",
    "                    target_action[i] = avg_action\n",
    "                    future_reward += self.target_critic_model.predict(\n",
    "                    [new_state, target_action])[0][0]\n",
    "                    target_action[i] = temp\n",
    "                ###########################################\n",
    "\n",
    "                reward += self.gamma * future_reward\n",
    "\n",
    "            cur_state = cur_state.reshape((1, 24))\n",
    "            action = action.reshape((1, 24))\n",
    "            reward = np.array(reward)\n",
    "            reward = reward.reshape((1, 1))\n",
    "\n",
    "            self.critic_model.fit([cur_state, action], reward, verbose=0)\n",
    "            return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we've collected <batch_size> number of samples, we train the actor-critic network on one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return False, \"error\"\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        reward = self._train_critic(samples)\n",
    "        self._train_actor(samples)\n",
    "        return True, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating target actor to reflect target policy (we do this by updating the weights of actor target network to reflect those of the updated actor network):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _update_actor_target(self):\n",
    "\n",
    "        actor_model_weights = self.actor_model.get_weights()\n",
    "        actor_target_weights = self.target_critic_model.get_weights()\n",
    "\n",
    "        for i in range(len(actor_target_weights)-2):\n",
    "            actor_target_weights[i] = \\\n",
    "            actor_model_weights[i].reshape(actor_target_weights[i].shape)\n",
    "        \n",
    "        actor_target_weights[6] = \\\n",
    "        np.mean(actor_model_weights[6], axis=0).reshape(actor_target_weights[6].shape)\n",
    "        actor_target_weights[7] = \\\n",
    "        np.mean(actor_model_weights[7], axis=0).reshape(actor_target_weights[7].shape)\n",
    "\n",
    "        self.target_critic_model.set_weights(actor_target_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating target critic model from itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _update_critic_target(self):\n",
    "\n",
    "        critic_model_weights = self.critic_model.get_weights()\n",
    "        critic_target_weights = self.target_critic_model.get_weights()\n",
    "\n",
    "        for i in range(len(critic_target_weights)):\n",
    "            critic_target_weights[i] = critic_model_weights[i]\n",
    "        self.target_critic_model.set_weights(critic_target_weights)\n",
    "        \n",
    "    def update_target(self):\n",
    "\n",
    "        self._update_actor_target()\n",
    "        self._update_critic_target()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get predictions from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def act(self, cur_state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return pad(self.env.action_space.sample().reshape(1, 4), 24)\n",
    "        return self.actor_model.predict(cur_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "\n",
    "Run, record and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    env = gym.make(\"BipedalWalker-v2\")\n",
    "    actor_critic = ActorCritic(env, sess)\n",
    "\n",
    "    NUM_ITERATIONS = 100\n",
    "\n",
    "    episode = 0\n",
    "    epochs = 0\n",
    "    episode_rewards = []\n",
    "    max_epochal_rewards = []\n",
    "    max_epochal_scores = []\n",
    "\n",
    "    for i in range(NUM_ITERATIONS):\n",
    "        print(\"Episode \", episode)\n",
    "        cur_state = env.reset()\n",
    "        action = env.action_space.sample()\n",
    "        done = False\n",
    "        epoch = 0\n",
    "        updated = False\n",
    "        cum_reward_epoch = 0\n",
    "        cum_reward_episode = 0\n",
    "        epochal_rewards = []\n",
    "        epochal_scores = []\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            if updated:\n",
    "                print(\"Epoch \", epoch, \"with reward \", cum_reward_epoch)\n",
    "                cum_reward_episode += cum_reward_epoch\n",
    "                epochal_rewards.append(cum_reward_epoch)\n",
    "                epoch += 1\n",
    "                cum_reward_epoch = 0\n",
    "                updated = False\n",
    "\n",
    "            env.render()\n",
    "            cur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "            action = actor_critic.act(cur_state)\n",
    "            action_taken = unpad(action, 4).reshape((4))\n",
    "            action_keras = action.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "            new_state, reward, done, _ = env.step(action_taken)\n",
    "            cum_reward_epoch += reward\n",
    "            new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "            actor_critic.remember(cur_state, action_keras, reward, new_state, done)\n",
    "            trained, score = actor_critic.train()\n",
    "            if trained:\n",
    "                actor_critic.update_target()\n",
    "                actor_critic.memory = []\n",
    "                updated = True\n",
    "                epochal_scores.append(score[0][0])\n",
    "\n",
    "            cur_state = new_state\n",
    "\n",
    "        episode += 1\n",
    "        epochs += epoch\n",
    "        episode_rewards.append(cum_reward_episode)\n",
    "        max_epochal_rewards.append(max(epochal_rewards))\n",
    "        max_epochal_scores.append(max(epochal_scores))\n",
    "\n",
    "    return episode, epochs, episode_rewards, max_epochal_rewards, max_epochal_scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x1, x2, y1, y2, y3 = main()\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(1, x1+1), y1)\n",
    "    plt.ylabel('Reward per episode')\n",
    "\n",
    "    #plt.subplot(2, 1, 2)\n",
    "    #plt.plot(range(1, x1+1), y2)\n",
    "    #plt.ylabel('Max. reward per epoch')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(range(1, x1+1), y3)\n",
    "    plt.ylabel('Max. score per epoch')\n",
    "\n",
    "    plt.savefig(\"max_action_100_iters.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
